# infer_server.py
import os
import time
import json
import zmq
import msgpack
import logging
import numpy as np
from .model_operator import LLMLoader, AltTensor

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)
logger = logging.getLogger("InferServer")


def alt_tensor_to_prompt(alt_tensor: AltTensor, trading_style: str = None) -> str:
    """
    å°† AltTensor ä¸­çš„ä¿¡æ¯è½¬æ¢ä¸ºäº¤æ˜“ agent çš„ prompt
    ç”¨äºå…¨è‡ªåŠ¨åŒ–äº¤æ˜“å†³ç­–
    
    Rust å‘é€çš„æ•°æ®æ ¼å¼ï¼š
    - data: æœ€æ–°ä¸€è¡Œçš„æ‰€æœ‰ç‰¹å¾å€¼ï¼ˆæµ®ç‚¹æ•°æ•°ç»„ï¼‰
    - metadata.col_names: æ‰€æœ‰åˆ—åï¼ˆJSON å­—ç¬¦ä¸²æ•°ç»„ï¼‰
    - metadata.price: å½“å‰ä»·æ ¼
    - metadata.pos_weight: å½“å‰ä»“ä½æƒé‡
    """
    metadata = alt_tensor.metadata
    
    # æå–å…³é”®ä¿¡æ¯
    price = metadata.get("price", "æœªçŸ¥")
    pos_weight = metadata.get("pos_weight", "0.0")
    col_names_str = metadata.get("col_names", "[]")
    
    # è§£æåˆ—å
    try:
        col_names = json.loads(col_names_str) if col_names_str else []
    except Exception as e:
        logger.warning(f"[Agent] Failed to parse col_names: {e}")
        col_names = []
    
    # æå–ç‰¹å¾æ•°æ®ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
    data_values = alt_tensor.data.tolist()
    
    # éªŒè¯æ•°æ®é•¿åº¦åŒ¹é…
    if len(col_names) != len(data_values):
        logger.warning(
            f"[Agent] Column count mismatch: {len(col_names)} columns but {len(data_values)} values. "
            f"Using indices for unnamed columns."
        )
    
    # æ„å»º prompt
    prompt_parts = []
    
    # åŸºç¡€è§’è‰²è®¾å®š
    prompt_parts.append("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡åŒ–äº¤æ˜“å‘˜ï¼Œéœ€è¦æ ¹æ®å®æ—¶å¸‚åœºæ•°æ®åšå‡ºäº¤æ˜“å†³ç­–ã€‚")
    prompt_parts.append("")
    
    # äº¤æ˜“é£æ ¼å®šä¹‰ï¼ˆå¦‚æœæä¾›ï¼‰
    if trading_style:
        prompt_parts.append("## äº¤æ˜“é£æ ¼")
        prompt_parts.append(trading_style)
        prompt_parts.append("")
    
    # å½“å‰å¸‚åœºä¿¡æ¯
    prompt_parts.append("## å½“å‰å¸‚åœºä¿¡æ¯")
    prompt_parts.append(f"- äº¤æ˜“å¯¹: DOGE_USDT_PERP")
    prompt_parts.append(f"- å½“å‰ä»·æ ¼: {price}")
    prompt_parts.append(f"- å½“å‰ä»“ä½æƒé‡: {pos_weight} (-1åˆ°1ä¹‹é—´ï¼Œ1è¡¨ç¤ºæ»¡ä»“åšå¤šï¼Œ0è¡¨ç¤ºç©ºä»“ï¼Œ-1è¡¨ç¤ºæ»¡ä»“åšç©º)")
    prompt_parts.append("")
    
    # ç‰¹å¾æ•°æ® - åˆ†ç±»å±•ç¤º
    if col_names and len(col_names) == len(data_values):
        # åˆ†ç±»ç‰¹å¾ï¼šåŸå§‹ç‰¹å¾ vs z-score ç‰¹å¾
        raw_features = []
        zscore_features = []
        timestamp_idx = -1
        
        for i, (col_name, value) in enumerate(zip(col_names, data_values)):
            if col_name == "timestamp":
                timestamp_idx = i
                continue
            elif col_name.startswith("z_"):
                # z-score ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–åçš„ç‰¹å¾ï¼Œé€šå¸¸åœ¨ -3 åˆ° 3 ä¹‹é—´ï¼‰
                zscore_features.append((col_name, value))
            else:
                # åŸå§‹ç‰¹å¾
                raw_features.append((col_name, value))
        
        # æ˜¾ç¤ºåŸå§‹ç‰¹å¾
        if raw_features:
            prompt_parts.append("## åŸå§‹å¸‚åœºç‰¹å¾æ•°æ®")
            for col_name, value in raw_features:
                prompt_parts.append(f"- {col_name}: {value:.6f}")
            prompt_parts.append("")
        
        # æ˜¾ç¤º z-score ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–ç‰¹å¾ï¼Œæ›´æ˜“äºåˆ†æï¼‰
        if zscore_features:
            prompt_parts.append("## æ ‡å‡†åŒ–ç‰¹å¾æ•°æ® (Z-Score)")
            prompt_parts.append("(è¿™äº›ç‰¹å¾å·²ç»è¿‡æ ‡å‡†åŒ–å¤„ç†ï¼Œæ•°å€¼é€šå¸¸åœ¨ -3 åˆ° 3 ä¹‹é—´)")
            prompt_parts.append("(ç»å¯¹å€¼è¶Šå¤§è¡¨ç¤ºåç¦»å‡å€¼è¶Šè¿œï¼Œæ­£å€¼è¡¨ç¤ºé«˜äºå‡å€¼ï¼Œè´Ÿå€¼è¡¨ç¤ºä½äºå‡å€¼)")
            prompt_parts.append("")
            for col_name, value in zscore_features:
                # æ·»åŠ è§£é‡Šæ€§æ ‡è®°
                abs_value = abs(value)
                if abs_value > 2.0:
                    significance = "âš ï¸ æ˜¾è‘—åç¦»"
                elif abs_value > 1.0:
                    significance = "ğŸ“Š ä¸­ç­‰åç¦»"
                else:
                    significance = "âœ“ æ¥è¿‘å‡å€¼"
                prompt_parts.append(f"- {col_name}: {value:.4f} {significance}")
            prompt_parts.append("")
        
        # å¦‚æœæ²¡æœ‰åˆ†ç±»åˆ°ä»»ä½•ç‰¹å¾ï¼Œæ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾
        if not raw_features and not zscore_features:
            prompt_parts.append("## å¸‚åœºç‰¹å¾æ•°æ®")
            for i, (col_name, value) in enumerate(zip(col_names, data_values)):
                if col_name != "timestamp":
                    prompt_parts.append(f"- {col_name}: {value:.6f}")
            prompt_parts.append("")
    
    elif data_values:
        # å¦‚æœæ²¡æœ‰åˆ—åï¼Œä½¿ç”¨ç´¢å¼•
        prompt_parts.append("## å¸‚åœºç‰¹å¾æ•°æ®")
        prompt_parts.append(f"- ç‰¹å¾æ•°é‡: {len(data_values)}")
        for i, value in enumerate(data_values[:20]):  # æ˜¾ç¤ºå‰20ä¸ª
            prompt_parts.append(f"  ç‰¹å¾[{i}]: {value:.6f}")
        if len(data_values) > 20:
            prompt_parts.append(f"  ... (å…± {len(data_values)} ä¸ªç‰¹å¾)")
        prompt_parts.append("")
    
    # ä»»åŠ¡è¦æ±‚ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼ŒåŠ å¿« LLM å“åº”é€Ÿåº¦ï¼‰
    prompt_parts.append("## ä»»åŠ¡è¦æ±‚")
    prompt_parts.append("è¯·æ ¹æ®ä»¥ä¸Šå¸‚åœºæ•°æ®åšå‡ºäº¤æ˜“å†³ç­–ã€‚")
    prompt_parts.append("")
    prompt_parts.append("è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ï¼‰ï¼šPOSITION_SIZE=<æ•°å€¼>")
    prompt_parts.append("- æ•°å€¼èŒƒå›´ï¼š-1åˆ°1ï¼ˆ1=æ»¡ä»“åšå¤šï¼Œ0=ç©ºä»“ï¼Œ-1=æ»¡ä»“åšç©ºï¼‰")
    prompt_parts.append("- ç¤ºä¾‹ï¼šPOSITION_SIZE=0.5 æˆ– POSITION_SIZE=-0.3")
    prompt_parts.append("")
    prompt_parts.append("è¯·ç›´æ¥è¾“å‡º POSITION_SIZE=... æ ¼å¼ï¼š")
    
    return "\n".join(prompt_parts)


def predict_alt_tensor(alt_tensor: AltTensor, model_loader: LLMLoader) -> dict:
    try:
        pred_tensor = model_loader.predict(alt_tensor)
        return pred_tensor.to_dict()
    except Exception as e:
        logger.exception(f"[Error] Model prediction failed: {e}")
        return AltTensor(
            timestamp=int(time.time() * 1000),
            data=np.zeros([1], dtype=np.float32),
            shape=[1],
            metadata={"error": "ERROR_PREDICTION_FAILED", "error_msg": str(e)}
        ).to_dict()


def load_models_for_port(config_path: str, port: int) -> dict:
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found: {config_path}")

    with open(config_path, "r") as f:
        config = json.load(f)

    model_map = {}
    for c in config:
        if c.get("port") == port:
            model_id = c["model_id"]
            
            # åªæ”¯æŒLLMæ¨¡å‹
            llm_config = {
                "llm_provider": c.get("llm_provider", "gemini"),
                "api_key": c.get("api_key", ""),
                "model_name": c.get("model_name", "gemini-2.0-flash-exp")
            }
            model_map[model_id] = LLMLoader(llm_config)
            logger.info(f"[Init] Loaded LLM model '{model_id}' ({llm_config['llm_provider']}) for port {port}")

    logger.info(f"[Init] Total {len(model_map)} models loaded for port {port}")
    return model_map


def run_server(port: int, config_path: str, trading_style: str = None):
    logger.info(f"[Agent] ğŸš€ Starting server on port {port}")
    models = load_models_for_port(config_path, port)
    logger.info(f"[Agent] âœ… Loaded {len(models)} model(s)")

    ctx = zmq.Context()
    socket = ctx.socket(zmq.REP)
    socket.bind(f"tcp://127.0.0.1:{port}")
    logger.info(f"[Agent] ğŸ”Œ ZMQ bound to tcp://127.0.0.1:{port}")
    logger.info(f"[Agent] â³ Waiting for data from Rust MCP server...")

    while True:
        raw = socket.recv()
        try:
            # æ¥æ”¶çš„æ•°æ®æ ¼å¼: (timestamp, data_raw, shape, metadata)
            # ä¸¥æ ¼éµå®ˆ Rust AltTensor å®šä¹‰
            unpacked = msgpack.unpackb(raw, raw=False)
            
            if isinstance(unpacked, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºå…ƒç»„æ ¼å¼
                timestamp = unpacked.get("timestamp", int(time.time() * 1000))
                data_raw = unpacked.get("data", [])
                shape = unpacked.get("shape", [1])
                metadata = unpacked.get("metadata", {})
            else:
                # æ ‡å‡†æ ¼å¼: (timestamp, data_raw, shape, metadata)
                timestamp, data_raw, shape, metadata = unpacked

            model_id = metadata.get("model_id", "")
            logger.info(f"[Agent] ğŸ“¨ Received request | model_id={model_id}")

            if model_id not in models:
                logger.error(f"[Agent] âŒ Model '{model_id}' not found on port {port}")
                fallback = AltTensor(
                    timestamp=int(time.time() * 1000),
                    data=np.zeros([1], dtype=np.float32),
                    shape=[1],
                    metadata={"error": "ERROR_MODEL_NOT_FOUND"}
                ).to_dict()
                socket.send(msgpack.packb(fallback, use_bin_type=True))
                continue

            # å°†æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå¿…é¡»æ˜¯æµ®ç‚¹æ•°ï¼‰
            data_np = np.array(data_raw, dtype=np.float32).reshape(shape)
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            if np.any(np.isnan(data_np)) or np.any(np.isinf(data_np)):
                logger.error(f"[Agent] âŒ Invalid input data for model_id={model_id}")
                fallback = AltTensor(
                    timestamp=int(time.time() * 1000),
                    data=np.zeros([1], dtype=np.float32),
                    shape=[1],
                    metadata={"error": "ERROR_INVALID_INPUT"}
                ).to_dict()
                socket.send(msgpack.packb(fallback, use_bin_type=True))
                continue

            # æ„é€ AltTensorï¼ˆä¸¥æ ¼éµå®ˆRustå®šä¹‰ï¼‰
            alt_tensor_input = AltTensor(
                timestamp=timestamp,
                data=data_np,
                shape=list(shape),
                metadata=metadata
            )

            # æ˜¾ç¤ºæ¥æ”¶åˆ°çš„å…³é”®æ•°æ®
            price = metadata.get("price", "N/A")
            pos_weight = metadata.get("pos_weight", "0.0")
            data_len = len(data_np)
            logger.info(f"[Agent] ğŸ“Š Received | price={price} | pos={pos_weight} | features={data_len}")
            
            # è‡ªåŠ¨å°† AltTensor ä¿¡æ¯è½¬æ¢ä¸º promptï¼ˆå…¨è‡ªåŠ¨åŒ–äº¤æ˜“ agentï¼‰
            # å¦‚æœ metadata ä¸­å·²ç»æœ‰ promptï¼Œåˆ™ä½¿ç”¨å·²æœ‰çš„ï¼›å¦åˆ™è‡ªåŠ¨ç”Ÿæˆ
            if "prompt" not in metadata or not metadata.get("prompt"):
                auto_prompt = alt_tensor_to_prompt(alt_tensor_input, trading_style=trading_style)
                metadata["prompt"] = auto_prompt
                # æ›´æ–° alt_tensor_input çš„ metadata
                alt_tensor_input.metadata = metadata
                logger.info(f"[Agent] ğŸ“ Generated prompt ({len(auto_prompt)} chars)")
            
            # LLMé¢„æµ‹
            logger.info(f"[Agent] ğŸ¤– Calling LLM...")
            start = time.time()
            result_dict = predict_alt_tensor(alt_tensor_input, models[model_id])
            latency = (time.time() - start) * 1000
            
            # æå–äº¤æ˜“å†³ç­–ä¿¡æ¯
            result_metadata = result_dict.get("metadata", {})
            response = result_metadata.get("response", "")
            cmd = result_metadata.get("cmd", "noop")
            inst = result_metadata.get("inst", "N/A")
            target_pos = result_metadata.get("target_position", result_metadata.get("pos_weight", "N/A"))
            
            # æ˜¾ç¤º LLM å“åº”æ‘˜è¦
            response_preview = response[:3000] + "..." if len(response) > 3000 else response
            logger.info(f"[Agent] ğŸ’¬ LLM Response: {response_preview}")
            
            # æ˜¾ç¤ºäº¤æ˜“å†³ç­–
            logger.info(f"[Agent] âœ… Decision | cmd={cmd} | inst={inst} | target_pos={target_pos} | latency={latency:.0f}ms")

            socket.send(msgpack.packb(result_dict, use_bin_type=True))

        except Exception as e:
            logger.error(f"[Agent] âŒ Exception: {e}")
            logger.exception(f"[Agent] Exception details:")
            fallback = AltTensor(
                timestamp=int(time.time() * 1000),
                data=np.zeros([1], dtype=np.float32),
                shape=[1],
                metadata={"error": "ERROR_EXCEPTION", "error_msg": str(e)}
            ).to_dict()
            socket.send(msgpack.packb(fallback, use_bin_type=True))


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="model_config.json")
    parser.add_argument("--port", type=int, required=True)
    args = parser.parse_args()
    run_server(args.port, args.config)
