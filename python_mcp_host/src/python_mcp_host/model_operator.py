import os
import json
import logging
import re
import numpy as np
from typing import Dict, List, Optional
import google.generativeai as genai

logger = logging.getLogger("ModelLoader")


# ============================
#  LLM Base Model
# ============================
class BaseLLM:
    def predict_text(self, prompt: str, **kwargs) -> str:
        raise NotImplementedError


# ============================
#  Gemini LLM
# ============================
class GeminiLLM(BaseLLM):
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash-lite"):
        if not api_key:
            raise ValueError("Gemini API key is required")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.model_name = model_name
        logger.info(f"[GeminiLLM] Initialized with model: {model_name}")

    def predict_text(self, prompt: str, generation_config: dict = None) -> str:
        try:
            if generation_config:
                response = self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(**generation_config)
                )
            else:
                response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.exception(f"[GeminiLLM] Error generating content: {e}")
            raise


# ============================
#  AltTensor å®šä¹‰ - ä¸¥æ ¼éµå®ˆRustç»“æ„
# ============================
class AltTensor:
    """
    AltTensor ä¸¥æ ¼éµå®ˆ Rust å®šä¹‰:
    - timestamp: u64
    - data: Vec<f32> (åªèƒ½æ˜¯æµ®ç‚¹æ•°å‘é‡)
    - shape: Vec<usize>
    - metadata: HashMap<String, String>
    """
    def __init__(self, timestamp: int, data: np.ndarray, shape: List[int], metadata: Dict[str, str]):
        self.timestamp = int(timestamp)
        # data å¿…é¡»æ˜¯æµ®ç‚¹æ•°æ•°ç»„
        if isinstance(data, (list, tuple)):
            data = np.array(data, dtype=np.float32)
        elif not isinstance(data, np.ndarray):
            raise TypeError(f"data must be numpy array or list, got {type(data)}")
        
        self.data = data.flatten().astype(np.float32)
        self.shape = [int(s) for s in shape]
        self.metadata = {str(k): str(v) for k, v in metadata.items()}  # ç¡®ä¿éƒ½æ˜¯å­—ç¬¦ä¸²

    def to_dict(self) -> Dict:
        return {
            "timestamp": self.timestamp,
            "data": self.data.tolist(),
            "shape": self.shape,
            "metadata": self.metadata
        }


# ============================
#  LLM å“åº”è§£æå™¨ - æå– Rust Mediator éœ€è¦çš„å­—æ®µ
# ============================
def parse_llm_response_for_mediator(response_text: str) -> Dict[str, str]:
    """
    è§£æ LLM å“åº”ï¼Œæå– Rust MCP Mediator éœ€è¦çš„å­—æ®µ
    æ”¯æŒçš„å‘½ä»¤: adjust_position, risk_alert, query, noop
    
    è¿”å›çš„ metadata å­—æ®µ:
    - cmd: å‘½ä»¤ç±»å‹
    - inst: äº¤æ˜“å¯¹åç§°ï¼ˆå¦‚ "DOGE_USDT_PERP"ï¼‰
    - target_position æˆ– pos_weight: ç›®æ ‡ä»“ä½æƒé‡ï¼ˆæµ®ç‚¹æ•°ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
    """
    metadata = {}
    
    # é¦–å…ˆå°è¯•è§£æ JSON æ ¼å¼çš„å“åº”
    json_match = re.search(r'\{[^{}]*"cmd"[^{}]*\}', response_text, re.DOTALL)
    if json_match:
        try:
            json_str = json_match.group(0)
            parsed = json.loads(json_str)
            if isinstance(parsed, dict):
                if "cmd" in parsed:
                    metadata["cmd"] = str(parsed["cmd"])
                if "inst" in parsed:
                    metadata["inst"] = str(parsed["inst"])
                if "target_position" in parsed:
                    metadata["target_position"] = str(parsed["target_position"])
                if "pos_weight" in parsed:
                    metadata["pos_weight"] = str(parsed["pos_weight"])
                logger.info(f"[Parser] Extracted JSON fields: {metadata}")
                return metadata
        except json.JSONDecodeError:
            pass
    
    # å¦‚æœæ²¡æœ‰ JSONï¼Œå°è¯•å…³é”®è¯åŒ¹é…
    response_lower = response_text.lower()
    
    # æ£€æµ‹å‘½ä»¤ç±»å‹
    if "adjust" in response_lower and "position" in response_lower:
        metadata["cmd"] = "adjust_position"
    elif "risk" in response_lower and "alert" in response_lower:
        metadata["cmd"] = "risk_alert"
    elif "query" in response_lower:
        metadata["cmd"] = "query"
    else:
        metadata["cmd"] = "noop"
    
    # æå–äº¤æ˜“å¯¹åç§°ï¼ˆå¸¸è§æ ¼å¼ï¼šXXX_USDT_PERP, XXX-USDT-PERP, XXX/USDTï¼‰
    inst_patterns = [
        r'([A-Z]+_[A-Z]+_PERP)',  # DOGE_USDT_PERP
        r'([A-Z]+-[A-Z]+-PERP)',   # DOGE-USDT-PERP
        r'([A-Z]+/[A-Z]+)',        # DOGE/USDT
    ]
    for pattern in inst_patterns:
        match = re.search(pattern, response_text, re.IGNORECASE)
        if match:
            inst = match.group(1).upper().replace('-', '_')
            if not inst.endswith('_PERP') and '_' in inst:
                inst = inst + '_PERP'
            metadata["inst"] = inst
            break
    
    # æå–ä»“ä½æƒé‡ï¼ˆæ•°å­—ï¼Œæ”¯æŒè´Ÿæ•°åšç©ºï¼ŒèŒƒå›´ -1 åˆ° 1ï¼‰
    # ä¼˜å…ˆåŒ¹é… POSITION_SIZE=-0.XX æˆ– POSITION_SIZE=0.XX æ ¼å¼ï¼ˆæ”¯æŒè´Ÿæ•°ï¼‰
    position_size_match = re.search(r'POSITION_SIZE\s*=\s*(-?[0-9]+\.?[0-9]*)', response_text, re.IGNORECASE)
    if position_size_match:
        weight_str = position_size_match.group(1)
        try:
            weight_val = float(weight_str)
            # å¦‚æœæ˜¯å¤§äº1æˆ–å°äº-1çš„æ•°å­—ï¼Œå¯èƒ½æ˜¯ç™¾åˆ†æ¯”ï¼Œè½¬æ¢ä¸ºå°æ•°
            if abs(weight_val) > 1.0:
                weight_val = weight_val / 100.0
            # é™åˆ¶åœ¨ -1 åˆ° 1 ä¹‹é—´ï¼ˆæ”¯æŒåšç©ºï¼‰
            weight_val = max(-1.0, min(1.0, weight_val))
            metadata["target_position"] = str(weight_val)
            # å¦‚æœæ‰¾åˆ°äº†ä»“ä½ä¿¡æ¯ï¼Œä¸” cmd è¿˜æ˜¯ noopï¼Œæ”¹ä¸º adjust_position
            if metadata.get("cmd") == "noop":
                metadata["cmd"] = "adjust_position"
        except ValueError:
            pass
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° POSITION_SIZEï¼Œå°è¯•å…¶ä»–æ ¼å¼ï¼ˆæ”¯æŒè´Ÿæ•°åšç©ºï¼‰
    if "target_position" not in metadata:
        weight_patterns = [
            r'(?:position|weight|target)[\s:]*(-?[0-9]+\.?[0-9]*)%?',  # æ”¯æŒè´Ÿæ•°
            r'(-?[0-9]+\.?[0-9]*)%?\s*(?:position|weight|target)',  # æ”¯æŒè´Ÿæ•°
            r'ä»“ä½[:\s]*(-?[0-9]+\.?[0-9]*)%?',  # æ”¯æŒè´Ÿæ•°
            r'(-?[0-9]+\.?[0-9]*)%?\s*ä»“ä½',  # æ”¯æŒè´Ÿæ•°
            r'åšå¤š[:\s]*([0-9]+\.?[0-9]*)%?',  # åšå¤š
            r'åšç©º[:\s]*([0-9]+\.?[0-9]*)%?',  # åšç©ºï¼ˆè½¬æ¢ä¸ºè´Ÿæ•°ï¼‰
            # æ›´å®½æ¾çš„æ¨¡å¼ï¼šæ•°å­—åé¢è·Ÿç€"ä»“ä½"ã€"åšå¤š"ã€"åšç©º"ç­‰å…³é”®è¯
            r'(-?[0-9]+\.?[0-9]+)\s*(?:ä»“ä½|åšå¤š|åšç©º|position|weight)',  # ç¡®ä¿æ•°å­—æ˜¯å°æ•°ï¼ˆåŒ…å«å°æ•°ç‚¹ï¼‰
        ]
        for i, pattern in enumerate(weight_patterns):
            match = re.search(pattern, response_text, re.IGNORECASE)
            if match:
                weight_str = match.group(1)
                try:
                    weight_val = float(weight_str)
                    
                    # éªŒè¯ï¼šæ’é™¤æ˜æ˜¾ä¸æ˜¯ä»“ä½æƒé‡çš„æ•°å­—ï¼ˆå¦‚å¹´ä»½ã€å¤§æ•´æ•°ç­‰ï¼‰
                    # ä»“ä½æƒé‡é€šå¸¸åœ¨ -100 åˆ° 100 ä¹‹é—´ï¼ˆç™¾åˆ†æ¯”ï¼‰æˆ– -1 åˆ° 1 ä¹‹é—´ï¼ˆå°æ•°ï¼‰
                    # å¦‚æœæ•°å­—ç»å¯¹å€¼å¤§äº 1000ï¼Œå¾ˆå¯èƒ½æ˜¯å¹´ä»½æˆ–å…¶ä»–æ— å…³æ•°å­—ï¼Œè·³è¿‡
                    if abs(weight_val) > 1000:
                        logger.debug(f"[Parser] Skipping unlikely weight value: {weight_val} (too large)")
                        continue
                    
                    # å¦‚æœæ˜¯"åšç©º"æ¨¡å¼ï¼Œè½¬æ¢ä¸ºè´Ÿæ•°
                    if i == len(weight_patterns) - 1:  # æœ€åä¸€ä¸ªæ¨¡å¼æ˜¯"åšç©º"
                        weight_val = -abs(weight_val)
                    
                    # å¦‚æœæ˜¯å¤§äº1æˆ–å°äº-1çš„æ•°å­—ï¼Œå¯èƒ½æ˜¯ç™¾åˆ†æ¯”ï¼Œè½¬æ¢ä¸ºå°æ•°
                    if abs(weight_val) > 1.0:
                        weight_val = weight_val / 100.0
                    
                    # é™åˆ¶åœ¨ -1 åˆ° 1 ä¹‹é—´ï¼ˆæ”¯æŒåšç©ºï¼‰
                    weight_val = max(-1.0, min(1.0, weight_val))
                    
                    # æœ€ç»ˆéªŒè¯ï¼šå¦‚æœè½¬æ¢åçš„å€¼æ¥è¿‘0ä¸”åŸå§‹å€¼å¾ˆå¤§ï¼Œå¯èƒ½æ˜¯è¯¯åŒ¹é…
                    if abs(weight_val) < 0.01 and abs(float(weight_str)) > 10:
                        logger.debug(f"[Parser] Skipping unlikely weight value: {weight_str} -> {weight_val} (suspicious conversion)")
                        continue
                    
                    metadata["target_position"] = str(weight_val)
                    # å¦‚æœæ‰¾åˆ°äº†ä»“ä½ä¿¡æ¯ï¼Œä¸” cmd è¿˜æ˜¯ noopï¼Œæ”¹ä¸º adjust_position
                    if metadata.get("cmd") == "noop":
                        metadata["cmd"] = "adjust_position"
                    break
                except ValueError:
                    continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äº¤æ˜“å¯¹ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆRust mediator éœ€è¦ inst å­—æ®µï¼‰
    # åªè¦ cmd æ˜¯ adjust_positionï¼Œå°±éœ€è¦ inst å­—æ®µ
    if "inst" not in metadata and metadata.get("cmd") == "adjust_position":
        metadata["inst"] = "DOGE_USDT_PERP"  # Rust mediator çš„é»˜è®¤å€¼
        logger.info("[Parser] No instrument found in response, using default: DOGE_USDT_PERP")
    
    if metadata:
        logger.info(f"[Agent] ğŸ” Parsed | {', '.join([f'{k}={v}' for k, v in metadata.items()])}")
    
    return metadata


# ============================
#  LLM Loader
# ============================
class LLMLoader:
    def __init__(self, config: Dict):
        """
        ä»é…ç½®åŠ è½½LLMæ¨¡å‹
        configåº”åŒ…å«: api_key, model_name, llm_providerç­‰
        """
        self.config = config
        self.llm_provider = config.get("llm_provider", "gemini").lower()
        self.api_key = config.get("api_key", "")
        self.model_name = config.get("model_name", "gemini-2.5-flash-lite")
        
        if not self.api_key:
            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            self.api_key = os.getenv("GEMINI_API_KEY", "")
        
        if not self.api_key:
            raise ValueError("API key is required. Set it in config or GEMINI_API_KEY environment variable")
        
        self.model = self._load_model()
        logger.info(f"[LLMLoader] Loaded {self.llm_provider} model: {self.model_name}")

    def _load_model(self) -> BaseLLM:
        if self.llm_provider == "gemini":
            return GeminiLLM(api_key=self.api_key, model_name=self.model_name)
        else:
            raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")

    def predict(self, alt_tensor: AltTensor) -> AltTensor:
        """
        LLMæ–‡æœ¬é¢„æµ‹
        æ–‡æœ¬é€šè¿‡ metadata["prompt"] ä¼ é€’
        è¾“å‡ºæ–‡æœ¬é€šè¿‡ metadata["response"] ä¼ é€’ï¼Œdata åŒ…å«ç¼–ç åçš„å“åº”
        """
        # ä»metadataè·å–prompt
        prompt = alt_tensor.metadata.get("prompt", "")
        if not prompt:
            # å¦‚æœæ²¡æœ‰promptï¼Œå°è¯•ä»dataæ„é€ ï¼ˆå°†æµ®ç‚¹æ•°è§£ç ä¸ºæ–‡æœ¬ï¼‰
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç¼–ç /è§£ç é€»è¾‘ï¼Œä½†é€šå¸¸promptåº”è¯¥åœ¨metadataä¸­
            logger.warning("[LLMLoader] No prompt in metadata, using empty prompt")
            prompt = ""
        
        # ä»metadataè·å–é¢å¤–çš„ç”Ÿæˆå‚æ•°
        temperature = float(alt_tensor.metadata.get("temperature", "0.7"))
        max_tokens = int(alt_tensor.metadata.get("max_tokens", "1000"))
        
        # è°ƒç”¨LLM
        try:
            response_text = self.model.predict_text(
                prompt,
                generation_config={
                    "temperature": temperature,
                    "max_output_tokens": max_tokens,
                }
            )
        except Exception as e:
            logger.exception(f"[LLMLoader] LLM prediction failed: {e}")
            response_text = f"ERROR: {str(e)}"
        
        # å°†å“åº”æ–‡æœ¬ç¼–ç ä¸ºæµ®ç‚¹æ•°æ•°ç»„ï¼ˆASCIIç¼–ç ï¼‰
        # æ¯ä¸ªå­—ç¬¦è½¬æ¢ä¸ºASCIIç ï¼Œç„¶åå½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        response_bytes = response_text.encode('utf-8')
        response_data = np.array([b / 255.0 for b in response_bytes], dtype=np.float32)
        
        # è§£æ LLM å“åº”ï¼Œæå– Rust Mediator éœ€è¦çš„å­—æ®µ
        mediator_fields = parse_llm_response_for_mediator(response_text)
        
        # æ„é€ è¿”å›AltTensor
        metadata = alt_tensor.metadata.copy()
        metadata["model_type"] = f"{self.llm_provider}_{self.model_name}"
        metadata["response"] = response_text  # å®Œæ•´å“åº”ä¿å­˜åœ¨metadataä¸­
        metadata["prompt"] = prompt  # ä¿å­˜åŸå§‹prompt
        
        # æ·»åŠ  Rust Mediator éœ€è¦çš„å­—æ®µ
        # å¦‚æœ LLM å“åº”ä¸­æ²¡æœ‰æå–åˆ° cmdï¼Œé»˜è®¤ä½¿ç”¨ "noop"
        if "cmd" not in mediator_fields:
            mediator_fields["cmd"] = "noop"
        
        # åˆå¹¶ mediator å­—æ®µåˆ° metadataï¼ˆç¡®ä¿éƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
        for key, value in mediator_fields.items():
            metadata[key] = str(value)
        
        logger.debug(f"[LLMLoader] Final metadata keys: {list(metadata.keys())}")
        
        return AltTensor(
            timestamp=alt_tensor.timestamp,
            data=response_data,
            shape=[len(response_data)],
            metadata=metadata
        )
